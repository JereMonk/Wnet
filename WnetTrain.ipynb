{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:05:26.847383Z",
     "start_time": "2021-06-15T10:05:26.836210Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:05:35.763401Z",
     "start_time": "2021-06-15T10:05:27.572437Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import h5py\n",
    "import PIL\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import Dense,MaxPool2D,Conv2D,Conv2DTranspose,Conv2D,Conv2DTranspose,BatchNormalization,Input,ReLU,Add,Lambda,LeakyReLU,Flatten,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "\n",
    "from importlib import reload \n",
    "from monk import Dataset\n",
    "   \n",
    "from PIL import Image \n",
    "import PIL \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:05:35.809311Z",
     "start_time": "2021-06-15T10:05:35.764838Z"
    }
   },
   "outputs": [],
   "source": [
    "import ncut_loss\n",
    "import build_Unet\n",
    "import build_Wnet\n",
    "import getData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:06:13.966574Z",
     "start_time": "2021-06-15T10:05:36.924563Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset_parts = Dataset.from_coco('/home/ubuntu/shared/data/meta/annotations/gold_standard/2021-06/parts_2021-06_train.json')\n",
    "to_keep = [\"bumper_back\",\"bumper_front\",\"door_back_left\",\"door_back_right\",\"door_front_left\",    \n",
    "\"door_front_right\",\"fender_back_left\",\"fender_back_right\",\"fender_front_left\",\"fender_front_right\"]    \n",
    "#dataset_filtered = dataset_parts.filter_images_with_cats(keep=to_keep).filter_cats(keep=to_keep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:06:40.012317Z",
     "start_time": "2021-06-15T10:06:39.922784Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/shared/data/meta/annotations/gold_standard/2021-06/parts_2021-06_train.json'\n",
    "gen = getData.get_generator(path,128,1,to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:06:42.915565Z",
     "start_time": "2021-06-15T10:06:42.874631Z"
    }
   },
   "outputs": [],
   "source": [
    "def rescale(image):\n",
    "    return( (((image+1)/2)*255 ).astype(\"uint8\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:07:25.647284Z",
     "start_time": "2021-06-15T10:07:25.382265Z"
    }
   },
   "outputs": [],
   "source": [
    "items = gen.__getitem__(100)\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(rescale(items[0].numpy()))\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(rescale(items[1].numpy()))\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(rescale(items[2].numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:07:40.391409Z",
     "start_time": "2021-06-15T10:07:40.348865Z"
    }
   },
   "outputs": [],
   "source": [
    "image_ref = tf.expand_dims(gen.__getitem__(100)[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:06:46.765855Z",
     "start_time": "2021-06-15T10:06:45.663038Z"
    }
   },
   "outputs": [],
   "source": [
    "neighbor_filter=ncut_loss.neighbor_filter((128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:06:48.688839Z",
     "start_time": "2021-06-15T10:06:46.767362Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = build_Unet.build_Unet(K=15,stages = [1,2,3,4],filters = 64,type='encoder',input_size=128)\n",
    "decoder = build_Unet.build_Unet(K=15,stages = [1,2,3,4],filters = 64,type='decoder',input_size=128)\n",
    "wn = build_Wnet.Wnet(encoder,decoder,(128,128))\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.mean(y_true - y_pred)**2\n",
    "\n",
    "# Compile the model\n",
    "wn.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.003),\n",
    "    loss_fn_segmentation = ncut_loss.compute_soft_ncuts,\n",
    "    loss_fn_reconstruction = tf.keras.losses.MeanSquaredError()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.save_weights('ckpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:06:48.744346Z",
     "start_time": "2021-06-15T10:06:48.690394Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualise_seg(image,encoder):\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    seg = encoder(image)\n",
    "    ag = tf.math.argmax(seg, axis=-1, output_type=tf.dtypes.int64)\n",
    "    loss = ncut_loss.compute_soft_ncuts(image,seg,neighbor_filter)\n",
    "    res = wn(image).numpy()[0]\n",
    "    res = rescale(res)\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(ag[0])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(res)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:14:54.174135Z",
     "start_time": "2021-06-15T10:14:53.719443Z"
    }
   },
   "outputs": [],
   "source": [
    "visualise_seg(gen.__getitem__(300)[0],wn.encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:14:45.550197Z",
     "start_time": "2021-06-15T10:14:45.372086Z"
    }
   },
   "outputs": [],
   "source": [
    "seg = encoder(image_ref)\n",
    "ag = tf.math.argmax(seg, axis=-1, output_type=tf.dtypes.int64)[0]\n",
    "ag = ag.numpy()\n",
    "ag = ag *255 /ag.max()\n",
    "print(ag.shape)\n",
    "plt.imshow(ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:10:47.421166Z",
     "start_time": "2021-06-15T10:10:47.238374Z"
    }
   },
   "outputs": [],
   "source": [
    "seg = encoder(image_ref)\n",
    "ag = tf.math.argmax(seg, axis=-1, output_type=tf.dtypes.int64)[0]\n",
    "ag = ag.numpy()\n",
    "ag = ag *255 /ag.max()\n",
    "print(ag.shape)\n",
    "plt.imshow(ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:16:28.991792Z",
     "start_time": "2021-06-15T10:16:28.928864Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_learning_rate(step_counter,model,base_lr):\n",
    "    if(step_counter>=1000):\n",
    "        new_lr = base_lr/(10**(step_counter//1000))\n",
    "    else:\n",
    "        new_lr = base_lr\n",
    "    model.optimizer.lr = new_lr\n",
    "    \n",
    "def training(model,train_dataset,epochs,start_epoch,base_lr,ckpt_freq,ckpt_dir_path,log_dir_path,img_dir_path,image_ref):\n",
    "  \n",
    "    step_counter=0\n",
    "    writer = tf.summary.create_file_writer(log_dir_path)\n",
    "    \n",
    "    print(epochs)\n",
    "    print(start_epoch)\n",
    "    \n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        set_learning_rate(epoch,model,base_lr)\n",
    "\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        print(\"Learning rate\" +str(model.optimizer.lr))\n",
    "\n",
    "\n",
    "\n",
    "        ##TRAIN\n",
    "        total_train_loss_encoder = []\n",
    "        total_train_loss_decoder = []\n",
    "\n",
    "        for step, x_batch_train in enumerate(train_dataset):\n",
    "            step_counter+=1\n",
    "            set_learning_rate(step_counter,model,base_lr)\n",
    "\n",
    "            train_losses = model.train_step(x_batch_train)\n",
    "\n",
    "            train_losses_encoder=train_losses[\"loss_encoder\"]\n",
    "            train_losses_decoder=train_losses[\"loss_decoder\"]\n",
    "\n",
    "            total_train_loss_encoder.append(train_losses_encoder.numpy())\n",
    "            total_train_loss_decoder.append(train_losses_decoder.numpy())\n",
    "\n",
    "            if step_counter%ckpt_freq ==0:\n",
    "                ckpt_name=\"ckpt\"+str(step_counter)\n",
    "                model.save_weights(ckpt_dir_path)\n",
    "            if step_counter%1==0:\n",
    "                print(\"step \"+str(step) ) \n",
    "                print('enc loss ',train_losses_encoder)\n",
    "                print('dec loss ' ,train_losses_decoder)\n",
    "\n",
    "            if step_counter%50==0:\n",
    "                res = model(image_ref).numpy()[0]\n",
    "                res = rescale(res)\n",
    "                Image.fromarray(res).save(img_dir_path+\"/reconstruction_step\"+ str(step_counter)+\"_.png\")\n",
    "\n",
    "                seg = model.encoder(image_ref)\n",
    "                ag = tf.math.argmax(seg, axis=-1, output_type=tf.dtypes.int64)[0]\n",
    "                ag = ag.numpy()\n",
    "                ag = ag *255 /ag.max()\n",
    "                Image.fromarray(ag).convert(\"L\").save(img_dir_path+\"/segmentation_step\"+ str(step_counter)+\"_.png\")\n",
    "\n",
    "\n",
    "        final_train_loss_decoder = np.mean(np.array(total_train_loss_decoder))\n",
    "        final_train_loss_encoder = np.mean(np.array(total_train_loss_encoder))\n",
    "\n",
    "        print(\"TRAIN LOSS ENCODER : \", final_train_loss_encoder)\n",
    "        print(\"TRAIN LOSS DECODER : \", final_train_loss_decoder)\n",
    "\n",
    "\n",
    "\n",
    "     ##TEST\n",
    "    \"\"\"\n",
    "    print('START TEST')\n",
    "\n",
    "    total_test_loss_encoder = []\n",
    "    total_test_loss_decoder = []\n",
    "\n",
    "    for _, x_batch_test in enumerate(test_dataset):\n",
    "\n",
    "        test_losses = model.test_step(x_batch_test)\n",
    "        test_losses_encoder=test_losses[\"loss_encoder\"]\n",
    "        test_losses_decoder=test_losses[\"loss_decoder\"]\n",
    "\n",
    "        total_test_loss_encoder.append(test_losses_encoder)\n",
    "        total_test_loss_decoder.append(test_losses_decoder)\n",
    "\n",
    "    final_test_loss_decoder = np.mean(np.array(total_test_loss_decoder.numpy()))\n",
    "    final_test_loss_encoder = np.mean(np.array(total_test_loss_encoder.numpy()))\n",
    "\n",
    "    print(\"TEST LOSS ENCODER : \", final_test_loss_encoder)\n",
    "    print(\"TEST LOSS DECODER : \", final_test_loss_decoder)\n",
    "\n",
    "    \"\"\"\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('training loss encoder', final_train_loss_encoder, step=epoch)\n",
    "        tf.summary.scalar('training loss decoder', final_train_loss_decoder, step=epoch)\n",
    "        tf.summary.scalar('training loss', final_train_loss_encoder+final_train_loss_decoder, step=epoch)\n",
    "\n",
    "        tf.summary.scalar('test loss encoder', final_test_loss_encoder, step=epoch)\n",
    "        tf.summary.scalar('test loss decoder', final_test_loss_decoder, step=epoch)\n",
    "        tf.summary.scalar('test loss', final_test_loss_decoder+final_test_loss_encoder, step=epoch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-15T10:16:27.167Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=5000\n",
    "base_lr=0.01\n",
    "ckpt_freq=1000\n",
    "start_epoch=0\n",
    "ckpt_dir_path = 'wnet1/cpkts'\n",
    "log_dir_path = 'wnet1/logs'\n",
    "img_dir_path = 'wnet1/imgs'\n",
    "image_ref =  tf.expand_dims(items[1],0)\n",
    "\n",
    "training(wn,gen,epochs,start_epoch,base_lr,ckpt_freq,ckpt_dir_path,log_dir_path,img_dir_path,image_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T10:03:14.744372Z",
     "start_time": "2021-06-15T10:03:14.448793Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'checkpoint' in os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"checkpoint\", 'r') as stream:\n",
    "    try:\n",
    "        data = (yaml.safe_load(stream))\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"model_checkpoint_path\"].replace('ckpt','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jeremy)",
   "language": "python",
   "name": "conda_jeremy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
